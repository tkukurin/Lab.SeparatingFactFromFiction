{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Expected /Users/toni/Documents/faks/seminar/models, but was not found\n"
     ]
    }
   ],
   "source": [
    "from util import gloveutil, const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = const.FileManager()\n",
    "glove_200 = gloveutil.load_glove(fm.GLOVE_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import reader\n",
    "df = reader.load_df_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "nwords = 10000\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras.preprocessing.text as Ktext\n",
    "import keras.preprocessing.sequence as Kseq\n",
    "\n",
    "xs = df.body_content\n",
    "tokenizer = Ktext.Tokenizer(num_words=nwords)\n",
    "tokenizer.fit_on_texts(xs)\n",
    "xs_seq = Kseq.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(xs), maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model_2_dim = 20\n",
    "\n",
    "x_train = xs_seq\n",
    "x_train_2 = np.zeros((xs_seq.shape[0], model_2_dim))\n",
    "y_train = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body_atreplies = df.body_atreplies.apply(lambda r: r if isinstance(r, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>media</th>\n",
       "      <th>parents</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_is_verified</th>\n",
       "      <th>user_name</th>\n",
       "      <th>body_atreplies</th>\n",
       "      <th>body_content</th>\n",
       "      <th>body_hashtags</th>\n",
       "      <th>body_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [children, created_at, favorite_count, id, label, media, parents, reply_count, retweet_count, user_handle, user_id, user_is_verified, user_name, body_atreplies, body_content, body_hashtags, body_links]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[type(r) != list for r in df.body_atreplies]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8776"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_index = dict()\n",
    "i = 0\n",
    "for atreplies in df.body_atreplies:\n",
    "    for atreply in filter(lambda x: x not in user_index, atreplies):\n",
    "        user_index[atreply] = i\n",
    "        i += 1\n",
    "len(user_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrix = np.zeros((len(df), len(user_index)))\n",
    "for row, v in enumerate(df.body_atreplies.head()):\n",
    "    for user in v:\n",
    "        users_matrix[row, user_index[user]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@21WIRE #FatherPed #VicarOfFiddly tune into http://bambuser.com/channel/Joe_Public ... ... from 5pm BST to see #Middlesbrough church shamed Happy Easter\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1554 5144  139    2    1 2046 2524  513   29    6  186 2081\n",
      " 1100  604]\n",
      "['your']\n",
      "(46754, 8776)\n"
     ]
    }
   ],
   "source": [
    "print(xs.iloc[0])\n",
    "print(xs_seq[0])\n",
    "print([k for k, v in tokenizer.word_index.items() if v == 96])\n",
    "print(users_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Layer\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_matrix = gloveutil.glove_to_embedding_matrix(\n",
    "    glove_200, tokenizer.word_index)\n",
    "X = Input(shape=(maxlen, ))\n",
    "embed = Embedding(\n",
    "  len(tokenizer.word_index), embedding_dim, weights=[embedding_matrix],\n",
    "  input_length=maxlen, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as Klay\n",
    "import keras.regularizers as Kreg\n",
    "from keras import Sequential\n",
    "\n",
    "model_seq = Sequential([\n",
    "    embed,\n",
    "    #Klay.Conv1D(filters=100, kernel_size=5, padding='same'),\n",
    "    #Klay.MaxPool1D(pool_size=2),\n",
    "    #Klay.Conv1D(filters=100, kernel_size=5, padding='same'),\n",
    "    Klay.LSTM(100),\n",
    "    #Klay.Flatten(),\n",
    "    Klay.Dense(100, activation='relu'),\n",
    "])\n",
    "\n",
    "model_seq2 = Sequential([\n",
    "    Klay.Dense(100, input_shape=(users_matrix.shape[1],)),\n",
    "    Klay.Dense(100)\n",
    "])\n",
    "\n",
    "concat = Klay.concatenate([model_seq.output, model_seq2.output])\n",
    "Y = Klay.Dense(4, activation='sigmoid')(concat)\n",
    "model_combined = Model([model_seq.input, model_seq2.input], Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "class Metrics(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_f1macros = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(\n",
    "            [self.validation_data[0], self.validation_data[1]]))).round()\n",
    "        val_targ = self.validation_data[2]\n",
    "        \n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='micro')\n",
    "        _val_f1_macro = f1_score(val_targ, val_predict, average='macro')\n",
    "        \n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_f1macros.append(_val_f1_macro)\n",
    "        \n",
    "        print(\"— f1: %f — f1_macro: %f\" % (_val_f1, _val_f1_macro))\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37403 samples, validate on 9351 samples\n",
      "Epoch 1/10\n",
      "37403/37403 [==============================] - 36s 972us/step - loss: 0.1227 - val_loss: 0.1991\n",
      "— f1: 0.276586 — f1_macro: 0.283090\n",
      "Epoch 2/10\n",
      "37403/37403 [==============================] - 35s 928us/step - loss: 0.1057 - val_loss: 0.2244\n",
      "— f1: 0.473432 — f1_macro: 0.340725\n",
      "Epoch 3/10\n",
      "  128/37403 [..............................] - ETA: 34s - loss: 0.1032"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37403/37403 [==============================] - 34s 908us/step - loss: 0.0926 - val_loss: 0.2929\n",
      "— f1: 0.416653 — f1_macro: 0.327935\n",
      "Epoch 4/10\n",
      "37403/37403 [==============================] - 34s 911us/step - loss: 0.0775 - val_loss: 0.2597\n",
      "— f1: 0.387441 — f1_macro: 0.315351\n",
      "Epoch 5/10\n",
      "37403/37403 [==============================] - 37s 1ms/step - loss: 0.0664 - val_loss: 0.2143\n",
      "— f1: 0.550541 — f1_macro: 0.366437\n",
      "Epoch 6/10\n",
      "37403/37403 [==============================] - 35s 929us/step - loss: 0.0532 - val_loss: 0.3060\n",
      "— f1: 0.641774 — f1_macro: 0.390703\n",
      "Epoch 7/10\n",
      "37403/37403 [==============================] - 36s 962us/step - loss: 0.0474 - val_loss: 0.3132\n",
      "— f1: 0.568427 — f1_macro: 0.370888\n",
      "Epoch 8/10\n",
      "37403/37403 [==============================] - 34s 918us/step - loss: 0.0335 - val_loss: 0.3324\n",
      "— f1: 0.669633 — f1_macro: 0.398045\n",
      "Epoch 9/10\n",
      "37403/37403 [==============================] - 39s 1ms/step - loss: 0.0300 - val_loss: 0.4177\n",
      "— f1: 0.683569 — f1_macro: 0.404932\n",
      "Epoch 10/10\n",
      "37403/37403 [==============================] - 37s 979us/step - loss: 0.0294 - val_loss: 0.3556\n",
      "— f1: 0.745866 — f1_macro: 0.420795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x145b1d2b0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_combined.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',)\n",
    "model_combined.fit(\n",
    "    [x_train, users_matrix], \n",
    "    to_categorical(y_train),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[metrics])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
