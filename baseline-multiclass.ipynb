{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim/Tfidf\n",
    "Gensim: 200-dim vectors, 15 epochs\n",
    "\n",
    "## Usage\n",
    "* [Doc2Vec tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import const, reader\n",
    "import gensim\n",
    "import sklearn.feature_extraction as sk_fe\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_baseline = const.FileManager(prefix='baseline-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = str(fm_baseline.model('gensim_200.model'))\n",
    "load_model = True \n",
    "\n",
    "\n",
    "def read_corpus(fname):\n",
    "  with open(fname) as f:\n",
    "    for i, line in enumerate(f):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(\n",
    "        gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "      \n",
    "def construct_doc2vec_model(data_loc, model_loc):\n",
    "  corpus = read_corpus(data_loc)\n",
    "  model = gensim.models.Doc2Vec(\n",
    "    corpus, size=200, workers=4, seed=const.SEED, iter=15)\n",
    "  model.save(model_loc)\n",
    "  return model\n",
    "\n",
    "\n",
    "#d2v_model = gensim.models.Doc2Vec.load(model_loc) if load_model\\\n",
    "#  else construct_doc2vec_model(const.Path.TWEETS_RAW_TEXT, model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "tfidf_vectorizer = sk_fe.text.TfidfVectorizer(\n",
    "  stop_words=sk_fe.stop_words.ENGLISH_STOP_WORDS)\n",
    "# content_tfidf = tfidf_vectorizer.fit_transform(df.input)\n",
    "#  reader.stream_lines(const.FileManager.TWEETS_RAW_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 s, sys: 577 ms, total: 31.5 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%time df = reader.load_df_multi(clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import logging\n",
    "import sklearn as s\n",
    "import sklearn.pipeline\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from util import logutil, evaluate\n",
    "logutil.ignore_warnings()\n",
    "\n",
    "kfolder = sk.model_selection.KFold(\n",
    "  n_splits=5, shuffle=True, random_state=const.SEED)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "X = np.array(df.body_content)\n",
    "y = np.array(df.label)\n",
    "\n",
    "lr_tfidf = sk.linear_model.LogisticRegression()\n",
    "tfidf_vectorizer = sk_fe.text.TfidfVectorizer(\n",
    "  stop_words=sk_fe.stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "tfidf_pipeline = sk.pipeline.make_pipeline(tfidf_vectorizer, lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:[0.96727623 0.96289167 0.96599294 0.96781093 0.96299465]\n",
      "DEBUG:__main__:[0.81837641 0.82741343 0.84737561 0.85169917 0.82574659]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('f1_micro',\n",
       "  array([0.96727623, 0.96289167, 0.96599294, 0.96781093, 0.96299465])),\n",
       " ('f1_macro',\n",
       "  array([0.81837641, 0.82741343, 0.84737561, 0.85169917, 0.82574659]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def pipeline_cross_eval(pipeline, scorings):\n",
    "  for scoring in scorings:\n",
    "    eval_ = sk.model_selection.cross_val_score(\n",
    "      pipeline, X, y, cv=kfolder.split(X), scoring=scoring)\n",
    "    log.debug(eval_)\n",
    "    yield (scoring, eval_)\n",
    "\n",
    "list(pipeline_cross_eval(tfidf_pipeline, ['f1_micro', 'f1_macro']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro 0.9653932839999999\n",
      "macro 0.834122242\n"
     ]
    }
   ],
   "source": [
    "print('micro', np.average([0.96727623, 0.96289167, 0.96599294, 0.96781093, 0.96299465]))\n",
    "print('macro', np.average([0.81837641, 0.82741343, 0.84737561, 0.85169917, 0.82574659]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec(sklearn.pipeline.TransformerMixin):\n",
    "  def __init__(self):\n",
    "    self.model = None\n",
    "  \n",
    "  def corpus(self, X):\n",
    "    for i, line in enumerate(X):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(\n",
    "        gensim.utils.simple_preprocess(line), [i])\n",
    "      \n",
    "  def fit(self, X, y):\n",
    "    self.model = gensim.models.Doc2Vec(\n",
    "      list(self.corpus(X)), size=200, workers=4, seed=const.SEED, iter=15)\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X):\n",
    "    return np.array([self.model.infer_vector(v) for v in X])\n",
    "\n",
    "lr_doc2vec = sk.linear_model.LogisticRegression()\n",
    "doc2vec = Doc2Vec()\n",
    "doc2vec_pipeline = sk.pipeline.make_pipeline(doc2vec, lr_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:[0.86439953 0.85477489 0.86226072 0.85819698 0.85754011]\n",
      "DEBUG:__main__:[0.23181714 0.23042551 0.23150913 0.23092196 0.23082681]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('f1_micro',\n",
       "  array([0.86439953, 0.85477489, 0.86226072, 0.85819698, 0.85754011])),\n",
       " ('f1_macro',\n",
       "  array([0.23181714, 0.23042551, 0.23150913, 0.23092196, 0.23082681]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.logger.setLevel(logging.ERROR)\n",
    "\n",
    "list(pipeline_cross_eval(doc2vec_pipeline, ['f1_micro', 'f1_macro']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro 0.8594344459999999\n",
      "macro 0.23110011\n"
     ]
    }
   ],
   "source": [
    "print('micro', np.average([0.86439953, 0.85477489, 0.86226072, 0.85819698, 0.85754011]))\n",
    "print('macro', np.average([0.23181714, 0.23042551, 0.23150913, 0.23092196, 0.23082681]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
