{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Expected /Users/toni/Documents/faks/seminar/models, but was not found\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import empath\n",
    "from util import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = const.FileManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('this is a sentence. Actually, two sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py:193: DeprecationWarning: [W003] Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'is',\n",
       " 'lemma': 'be',\n",
       " 'NE': '',\n",
       " 'POS_fine': 'VBZ',\n",
       " 'POS_coarse': 'VERB',\n",
       " 'arc': 'ROOT',\n",
       " 'modifiers': [{'word': 'this',\n",
       "   'lemma': 'this',\n",
       "   'NE': '',\n",
       "   'POS_fine': 'DT',\n",
       "   'POS_coarse': 'DET',\n",
       "   'arc': 'nsubj',\n",
       "   'modifiers': []},\n",
       "  {'word': 'sentence',\n",
       "   'lemma': 'sentence',\n",
       "   'NE': '',\n",
       "   'POS_fine': 'NN',\n",
       "   'POS_coarse': 'NOUN',\n",
       "   'arc': 'attr',\n",
       "   'modifiers': [{'word': 'a',\n",
       "     'lemma': 'a',\n",
       "     'NE': '',\n",
       "     'POS_fine': 'DT',\n",
       "     'POS_coarse': 'DET',\n",
       "     'arc': 'det',\n",
       "     'modifiers': []}]},\n",
       "  {'word': '.',\n",
       "   'lemma': '.',\n",
       "   'NE': '',\n",
       "   'POS_fine': '.',\n",
       "   'POS_coarse': 'PUNCT',\n",
       "   'arc': 'punct',\n",
       "   'modifiers': []}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.print_tree()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empath\n",
    "Lexicon substitute for the commercial LIWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = empath.Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'help': 0.0,\n",
       " 'office': 0.0,\n",
       " 'dance': 0.0,\n",
       " 'money': 0.0,\n",
       " 'wedding': 0.0,\n",
       " 'domestic_work': 0.0,\n",
       " 'sleep': 0.0,\n",
       " 'medical_emergency': 0.0,\n",
       " 'cold': 0.0,\n",
       " 'hate': 0.0,\n",
       " 'cheerfulness': 0.0,\n",
       " 'aggression': 0.0,\n",
       " 'occupation': 0.0,\n",
       " 'envy': 0.0,\n",
       " 'anticipation': 0.0,\n",
       " 'family': 0.0,\n",
       " 'vacation': 0.0,\n",
       " 'crime': 0.0,\n",
       " 'attractive': 0.0,\n",
       " 'masculine': 0.0,\n",
       " 'prison': 0.0,\n",
       " 'health': 0.0,\n",
       " 'pride': 0.0,\n",
       " 'dispute': 0.0,\n",
       " 'nervousness': 0.0,\n",
       " 'government': 0.0,\n",
       " 'weakness': 0.0,\n",
       " 'horror': 0.0,\n",
       " 'swearing_terms': 0.0,\n",
       " 'leisure': 0.0,\n",
       " 'suffering': 0.0,\n",
       " 'royalty': 0.0,\n",
       " 'wealthy': 0.0,\n",
       " 'tourism': 0.0,\n",
       " 'furniture': 0.0,\n",
       " 'school': 0.0,\n",
       " 'magic': 0.0,\n",
       " 'beach': 0.0,\n",
       " 'journalism': 0.0,\n",
       " 'morning': 0.0,\n",
       " 'banking': 0.0,\n",
       " 'social_media': 0.0,\n",
       " 'exercise': 0.0,\n",
       " 'night': 0.0,\n",
       " 'kill': 0.0,\n",
       " 'blue_collar_job': 0.0,\n",
       " 'art': 0.0,\n",
       " 'ridicule': 0.0,\n",
       " 'play': 0.0,\n",
       " 'computer': 0.0,\n",
       " 'college': 0.0,\n",
       " 'optimism': 0.0,\n",
       " 'stealing': 0.0,\n",
       " 'real_estate': 0.0,\n",
       " 'home': 0.0,\n",
       " 'divine': 0.0,\n",
       " 'sexual': 0.0,\n",
       " 'fear': 0.0,\n",
       " 'irritability': 0.0,\n",
       " 'superhero': 0.0,\n",
       " 'business': 0.0,\n",
       " 'driving': 0.0,\n",
       " 'pet': 0.0,\n",
       " 'childish': 0.0,\n",
       " 'cooking': 0.0,\n",
       " 'exasperation': 0.0,\n",
       " 'religion': 0.0,\n",
       " 'hipster': 0.0,\n",
       " 'internet': 0.0,\n",
       " 'surprise': 0.0,\n",
       " 'reading': 0.0,\n",
       " 'worship': 0.0,\n",
       " 'leader': 0.0,\n",
       " 'independence': 0.0,\n",
       " 'movement': 0.2,\n",
       " 'body': 0.0,\n",
       " 'noise': 0.0,\n",
       " 'eating': 0.0,\n",
       " 'medieval': 0.0,\n",
       " 'zest': 0.0,\n",
       " 'confusion': 0.0,\n",
       " 'water': 0.0,\n",
       " 'sports': 0.0,\n",
       " 'death': 0.0,\n",
       " 'healing': 0.0,\n",
       " 'legend': 0.0,\n",
       " 'heroic': 0.0,\n",
       " 'celebration': 0.0,\n",
       " 'restaurant': 0.0,\n",
       " 'violence': 0.2,\n",
       " 'programming': 0.0,\n",
       " 'dominant_heirarchical': 0.0,\n",
       " 'military': 0.0,\n",
       " 'neglect': 0.0,\n",
       " 'swimming': 0.0,\n",
       " 'exotic': 0.0,\n",
       " 'love': 0.0,\n",
       " 'hiking': 0.0,\n",
       " 'communication': 0.0,\n",
       " 'hearing': 0.0,\n",
       " 'order': 0.0,\n",
       " 'sympathy': 0.0,\n",
       " 'hygiene': 0.0,\n",
       " 'weather': 0.0,\n",
       " 'anonymity': 0.0,\n",
       " 'trust': 0.0,\n",
       " 'ancient': 0.0,\n",
       " 'deception': 0.0,\n",
       " 'fabric': 0.0,\n",
       " 'air_travel': 0.0,\n",
       " 'fight': 0.0,\n",
       " 'dominant_personality': 0.0,\n",
       " 'music': 0.0,\n",
       " 'vehicle': 0.0,\n",
       " 'politeness': 0.0,\n",
       " 'toy': 0.0,\n",
       " 'farming': 0.0,\n",
       " 'meeting': 0.0,\n",
       " 'war': 0.0,\n",
       " 'speaking': 0.0,\n",
       " 'listen': 0.0,\n",
       " 'urban': 0.0,\n",
       " 'shopping': 0.0,\n",
       " 'disgust': 0.0,\n",
       " 'fire': 0.0,\n",
       " 'tool': 0.0,\n",
       " 'phone': 0.0,\n",
       " 'gain': 0.0,\n",
       " 'sound': 0.0,\n",
       " 'injury': 0.0,\n",
       " 'sailing': 0.0,\n",
       " 'rage': 0.0,\n",
       " 'science': 0.0,\n",
       " 'work': 0.0,\n",
       " 'appearance': 0.0,\n",
       " 'valuable': 0.0,\n",
       " 'warmth': 0.0,\n",
       " 'youth': 0.0,\n",
       " 'sadness': 0.0,\n",
       " 'fun': 0.0,\n",
       " 'emotional': 0.0,\n",
       " 'joy': 0.0,\n",
       " 'affection': 0.0,\n",
       " 'traveling': 0.0,\n",
       " 'fashion': 0.0,\n",
       " 'ugliness': 0.0,\n",
       " 'lust': 0.0,\n",
       " 'shame': 0.0,\n",
       " 'torment': 0.0,\n",
       " 'economics': 0.0,\n",
       " 'anger': 0.0,\n",
       " 'politics': 0.0,\n",
       " 'ship': 0.0,\n",
       " 'clothing': 0.0,\n",
       " 'car': 0.0,\n",
       " 'strength': 0.0,\n",
       " 'technology': 0.0,\n",
       " 'breaking': 0.0,\n",
       " 'shape_and_size': 0.0,\n",
       " 'power': 0.0,\n",
       " 'white_collar_job': 0.0,\n",
       " 'animal': 0.0,\n",
       " 'party': 0.0,\n",
       " 'terrorism': 0.0,\n",
       " 'smell': 0.0,\n",
       " 'disappointment': 0.0,\n",
       " 'poor': 0.0,\n",
       " 'plant': 0.0,\n",
       " 'pain': 0.2,\n",
       " 'beauty': 0.0,\n",
       " 'timidity': 0.0,\n",
       " 'philosophy': 0.0,\n",
       " 'negotiate': 0.0,\n",
       " 'negative_emotion': 0.2,\n",
       " 'cleaning': 0.0,\n",
       " 'messaging': 0.0,\n",
       " 'competing': 0.0,\n",
       " 'law': 0.0,\n",
       " 'friends': 0.0,\n",
       " 'payment': 0.0,\n",
       " 'achievement': 0.0,\n",
       " 'alcohol': 0.0,\n",
       " 'liquid': 0.0,\n",
       " 'feminine': 0.0,\n",
       " 'weapon': 0.0,\n",
       " 'children': 0.0,\n",
       " 'monster': 0.0,\n",
       " 'ocean': 0.0,\n",
       " 'giving': 0.0,\n",
       " 'contentment': 0.0,\n",
       " 'writing': 0.0,\n",
       " 'rural': 0.0,\n",
       " 'positive_emotion': 0.0,\n",
       " 'musical': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.analyze('he hit the other person', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive/Negative words\n",
    "\n",
    "Positive words in ASCII, negative ISO-8859. `;` indicates comments, one empty line between comments and start of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_from_file(fname, encoding='UTF-8'):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        s = set()\n",
    "        include = lambda w: not (len(w.strip()) == 0 or w.startswith(';'))\n",
    "        return set(w.strip().lower() for w in f if include(w))\n",
    "    \n",
    "positive_words = words_from_file(fm.collection('positive-words.txt'))\n",
    "negative_words = words_from_file(fm.collection('negative-words.txt'), encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjclues\n",
    "\n",
    "Used vim to transfer them to JSON; removed 'polarity' and 'mpqpolarity' since only one entry uses these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "subjclues = pd.read_json(fm.collection('subjclues.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>pos1</th>\n",
       "      <th>priorpolarity</th>\n",
       "      <th>stemmed1</th>\n",
       "      <th>type</th>\n",
       "      <th>word1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>adj</td>\n",
       "      <td>negative</td>\n",
       "      <td>n</td>\n",
       "      <td>weaksubj</td>\n",
       "      <td>abandoned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>noun</td>\n",
       "      <td>negative</td>\n",
       "      <td>n</td>\n",
       "      <td>weaksubj</td>\n",
       "      <td>abandonment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>verb</td>\n",
       "      <td>negative</td>\n",
       "      <td>y</td>\n",
       "      <td>weaksubj</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>verb</td>\n",
       "      <td>negative</td>\n",
       "      <td>y</td>\n",
       "      <td>strongsubj</td>\n",
       "      <td>abase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>anypos</td>\n",
       "      <td>negative</td>\n",
       "      <td>y</td>\n",
       "      <td>strongsubj</td>\n",
       "      <td>abasement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len    pos1 priorpolarity stemmed1        type        word1\n",
       "0    1     adj      negative        n    weaksubj    abandoned\n",
       "1    1    noun      negative        n    weaksubj  abandonment\n",
       "2    1    verb      negative        y    weaksubj      abandon\n",
       "3    1    verb      negative        y  strongsubj        abase\n",
       "4    1  anypos      negative        y  strongsubj    abasement"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjclues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction as fe\n",
    "\n",
    "sentence = \"this is a test\"\n",
    "dv = fe.DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.text as kpt\n",
    "tok = kpt.Tokenizer()\n",
    "t1, t2 = \"this is the first text\", \"some other second document\"\n",
    "tok.fit_on_texts([t1,t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'this']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sequence_length = x.shape[1]\n",
    "vocabulary_size = len(vocabulary_inv)\n",
    "embedding_dim = 256\n",
    "filter_sizes = [1, 1]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 30\n",
    "\n",
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(\n",
    "    input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(\n",
    "    num_filters, kernel_size=(filter_sizes[0], embedding_dim), \n",
    "    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(\n",
    "    num_filters, kernel_size=(filter_sizes[1], embedding_dim), \n",
    "    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(\n",
    "    num_filters, kernel_size=(filter_sizes[2], embedding_dim), \n",
    "    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(\n",
    "    sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(\n",
    "    sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(\n",
    "    sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'weights.{epoch:03d}-{val_acc:.4f}.hdf5', \n",
    "    monitor='val_acc', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "    callbacks=[checkpoint], validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
