{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import empath\n",
    "from util import const, reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 6.77 s, total: 1min 35s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = reader.load_df(clean=True)\n",
    "fm = const.FileManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.1 ms, sys: 13.7 ms, total: 69.8 ms\n",
      "Wall time: 73 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py:193: DeprecationWarning: [W003] Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word': '@pammaysmcdonald',\n",
       " 'lemma': '@pammaysmcdonald',\n",
       " 'NE': 'GPE',\n",
       " 'POS_fine': 'NN',\n",
       " 'POS_coarse': 'NOUN',\n",
       " 'arc': 'ROOT',\n",
       " 'modifiers': [{'word': '@docrocktex26',\n",
       "   'lemma': '@docrocktex26',\n",
       "   'NE': '',\n",
       "   'POS_fine': 'CC',\n",
       "   'POS_coarse': 'CCONJ',\n",
       "   'arc': 'pobj',\n",
       "   'modifiers': [{'word': '@chesterbadger3',\n",
       "     'lemma': '@chesterbadger3',\n",
       "     'NE': '',\n",
       "     'POS_fine': 'NN',\n",
       "     'POS_coarse': 'NOUN',\n",
       "     'arc': 'amod',\n",
       "     'modifiers': []}]},\n",
       "  {'word': '@NPR',\n",
       "   'lemma': '@npr',\n",
       "   'NE': '',\n",
       "   'POS_fine': 'RB',\n",
       "   'POS_coarse': 'ADV',\n",
       "   'arc': 'punct',\n",
       "   'modifiers': []}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "%time doc = nlp(df.iloc[1].body_content)\n",
    "doc.print_tree()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.15 s, sys: 64.8 ms, total: 5.21 s\n",
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%time xs_normalized = df.body_content.apply(data_process.TweetNormalizer().transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time xs_parsed = xs_normalized.apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empath\n",
    "Lexicon substitute for the commercial LIWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movement', 0.2),\n",
       " ('violence', 0.2),\n",
       " ('pain', 0.2),\n",
       " ('negative_emotion', 0.2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = empath.Empath()\n",
    "s = 'he hit the other person'\n",
    "l = lexicon.analyze(s, normalize=True)\n",
    "[(k,v) for k, v in l.items() if v > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word collections\n",
    "\n",
    "`;` indicates comments, one empty line between comments and start of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative-words.txt       verbs-factive.txt        verbs-nonimplicative.txt\n",
      "positive-words.txt       verbs-implicative.txt    verbs-reporting.txt\n",
      "subjclues.json           verbs-nonfactive.txt\n",
      "./collections/negative-words.txt: ISO-8859 text\n"
     ]
    }
   ],
   "source": [
    "!ls ./collections/\n",
    "!file ./collections/negative-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_from_file(fname, encoding='UTF-8'):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        s = set()\n",
    "        include = lambda w: not (len(w.strip()) == 0 or w.startswith(';'))\n",
    "        return set(w.strip().lower() for w in f if include(w))\n",
    "\n",
    "fnames = [\n",
    "    'negative-words.txt', 'verbs-factive.txt', 'verbs-nonimplicative.txt', \n",
    "    'positive-words.txt', 'verbs-implicative.txt', 'verbs-reporting.txt']\n",
    "\n",
    "words = {}\n",
    "for fname in fnames:\n",
    "    key = fname[:fname.index('.')]\n",
    "    path = fm.collection(fname)\n",
    "    \n",
    "    try:\n",
    "        words[key] = words_from_file(path)\n",
    "    except:\n",
    "        words[key] = words_from_file(path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjclues\n",
    "\n",
    "Used vim to transfer them to JSON; removed 'polarity' and 'mpqpolarity' since only one entry uses these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_subjclues = pd.read_json(fm.collection('subjclues.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>pos1</th>\n",
       "      <th>priorpolarity</th>\n",
       "      <th>stemmed1</th>\n",
       "      <th>type</th>\n",
       "      <th>word1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>adj</td>\n",
       "      <td>negative</td>\n",
       "      <td>n</td>\n",
       "      <td>weaksubj</td>\n",
       "      <td>abandoned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>noun</td>\n",
       "      <td>negative</td>\n",
       "      <td>n</td>\n",
       "      <td>weaksubj</td>\n",
       "      <td>abandonment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>verb</td>\n",
       "      <td>negative</td>\n",
       "      <td>y</td>\n",
       "      <td>weaksubj</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>verb</td>\n",
       "      <td>negative</td>\n",
       "      <td>y</td>\n",
       "      <td>strongsubj</td>\n",
       "      <td>abase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>anypos</td>\n",
       "      <td>negative</td>\n",
       "      <td>y</td>\n",
       "      <td>strongsubj</td>\n",
       "      <td>abasement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len    pos1 priorpolarity stemmed1        type        word1\n",
       "0    1     adj      negative        n    weaksubj    abandoned\n",
       "1    1    noun      negative        n    weaksubj  abandonment\n",
       "2    1    verb      negative        y    weaksubj      abandon\n",
       "3    1    verb      negative        y  strongsubj        abase\n",
       "4    1  anypos      negative        y  strongsubj    abasement"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subjclues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjclues_set = set(df_subjclues.word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction as fe\n",
    "\n",
    "sentence = \"this is a test\"\n",
    "dv = fe.DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  2.,   1.],\n",
       "        [  0., 200.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.fit_transform([{'x': 2, 'y': 1}, {'y': 200}]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%capture is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%capture words.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dict_features(tokens, feature_set):\n",
    "    ret = defaultdict(int) #dict((f, 0) for f in feature_set)\n",
    "    for t in tokens:\n",
    "        if t in feature_set:\n",
    "            ret[t] += 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "feature_set = set(np.reshape(words.values(), -1))\n",
    "feature_set = feature_set.union(subjclues_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<class 'int'>, {'insolent': 1})]\n",
      "[['this', 'is', 'a', 'test', '@user', 'and', 'a', 'sentence', 'insolent', 'gutless']]\n",
      "['this', 'is', 'a', 'test', '@user', 'and', 'a', 'sentence', 'insolent', 'gutless']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from util import data_process, functions\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "\n",
    "\n",
    "class TokenizerTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalizer = data_process.TweetNormalizer()\n",
    "        self.regex_word_tokenizer = RegexpTokenizer(r'[@]?\\w+')\n",
    "        stemmer = PorterStemmer()\n",
    "        stem = lambda ws: [stemmer.stem(w) for w in ws]\n",
    "        self.pipeline = util.compose(\n",
    "            self.normalizer.transform,\n",
    "            self.regex_word_tokenizer.tokenize,\n",
    "            # 10 times faster without stemming \n",
    "            # stem,\n",
    "        )\n",
    "        \n",
    "        self.fit = functions.const(self)\n",
    "        self.transform = self.pipeline\n",
    "\n",
    "\n",
    "def tokenize(xs):\n",
    "    regex_word_tokenizer = RegexpTokenizer(r'[@]?\\w+')\n",
    "    xs = data_process.normalize(xs)\n",
    "    pipeline = data_process.compose(\n",
    "        regex_word_tokenizer.tokenize,)\n",
    "    return map(pipeline, xs)\n",
    "\n",
    "\n",
    "xs_tok = tokenize(['this is a TEST. @user and a sentence. insolent, gutless'])\n",
    "xs_feat = map(lambda t: dict_features(t, feature_set), xs_tok)\n",
    "print(list(xs_feat))\n",
    "xs_tok = tokenize(['this is a TEST. @user and a sentence. insolent, gutless'])\n",
    "print(list(xs_tok))\n",
    "print(TokenizerTransformer().transform('this is a TEST. @user and a sentence. insolent, gutless'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.53 s, sys: 77.8 ms, total: 6.6 s\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweet_tokenizer = TokenizerTransformer()\n",
    "full_data_count_vectorizer = fe.text.CountVectorizer(\n",
    "    analyzer=tweet_tokenizer.transform)\n",
    "full_data_count_vectorizer.fit_transform(df.body_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_transformed = df.body_content.apply(TokenizerTransformer().transform)\n",
    "text_lens = texts_transformed.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words = set([i for sublist in texts_transformed.values.tolist() for i in sublist])\n",
    "feature_set_limited = feature_set.intersection(uniq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6887\n",
      "4114\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_set))\n",
    "print(len(feature_set_limited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.pipeline as pipe\n",
    "from util import pipeutil\n",
    "\n",
    "def lexical_feat_pipeline():\n",
    "    dict_vectorizer_lex = fe.DictVectorizer()\n",
    "    dict_vectorizer_empath = fe.DictVectorizer()\n",
    "    \n",
    "    lex_empath_pipe = pipeutil.union(\n",
    "        pipeutil.pipe(\n",
    "            lambda x: dict_features(x, feature_set),\n",
    "            dict_vectorizer_lex),\n",
    "        pipeutil.pipe(\n",
    "            lexicon.analyze,\n",
    "            dict_vectorizer_empath\n",
    "        ))\n",
    "    \n",
    "    return lex_empath_pipe\n",
    "\n",
    "\n",
    "xs_tok = tokenize(df.body_content.iloc[:4])\n",
    "pipeline = lexical_feat_pipeline()\n",
    "print(pipeline.fit_transform(list(xs_tok)).todense())\n",
    "print(pipeline.transform(list(tokenize(['insolent']))).todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.pipeutil import GlobalTransform\n",
    "\n",
    "\n",
    "def network_features(df):\n",
    "    maybe_empty_list = lambda obj: obj if isinstance(obj, list) else []\n",
    "    filter_tweeting_user = lambda atreplies, username: [\n",
    "        (r, 1) for r in maybe_empty_list(atreplies) \n",
    "        if r[1:] != username]\n",
    "    \n",
    "    return map(dict, \n",
    "        (filter_tweeting_user(atreplies, username) \n",
    "        for atreplies, username in zip(df.body_atreplies, df.user_handle)))\n",
    "\n",
    "def network_feat_pipeline():\n",
    "    dict_vectorizer = fe.DictVectorizer()\n",
    "    return pipeutil.pipe(\n",
    "        GlobalTransform(network_features), \n",
    "        dict_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'@el_pais': 1},\n",
       " {},\n",
       " {},\n",
       " {'@clarincom': 1,\n",
       "  '@la100fm': 1,\n",
       "  '@telefecom': 1,\n",
       "  '@bairesdirecto': 1,\n",
       "  '@dariobarassi': 1},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'@Ebooksreport': 1, '@USATODAY': 1},\n",
       " {'@guardian': 1}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(network_features(df.iloc[20:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text as kpt\n",
    "\n",
    "tok = kpt.Tokenizer()\n",
    "texts = \"this is the first text\", \"some other second document\"\n",
    "tok.fit_on_texts(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@user', 'a', 'korean', 'american', 'man', 'detained', 'in', 'north', 'korea', 'has', 'reportedly', 'confessed', 'to', 'trying', '@url'], ['@user', '@user', '@user', '@user', 'meanwhile', 'culture', 'economic', 'shifts', 'left', 'both', 'irrelevant', 'but', 'still', 'sizeable', 'demographics']]\n"
     ]
    }
   ],
   "source": [
    "print(util.compose(tokenize, list)(df.body_content.head(n=2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 2.75 s, total: 1min 18s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from util import gloveutil\n",
    "\n",
    "# fit CountVectorizer on entire dataset and later use it for\n",
    "# word-to-index mapping?\n",
    "\n",
    "glove_200 = gloveutil.load_glove(const.FileManager.GLOVE_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 493 ms, total: 897 ms\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "embedding_matrix = gloveutil.glove_to_embedding_matrix(\n",
    "    glove_200, full_data_count_vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_cues_model(input_shape):\n",
    "    #model = Sequential()\n",
    "    #model.add(Dense(100, input_shape=input_shape, activation='relu'))\n",
    "    #model.add(Dense(100, input_shape=input_shape, activation='relu'))\n",
    "    xs = keras.Input(shape=input_shape)\n",
    "    ys = keras.layers.Dense(100)(xs)\n",
    "    ys = keras.layers.Dense(100)(ys)\n",
    "    return xs, ys, Model(xs, ys)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from util import pipeutil\n",
    "\n",
    "def sequence_model(embedding_matrix, word_index, max_sequence_length):\n",
    "    \"\"\"\n",
    "    embedding_matrix: matrix of 200-dim GloVe embeddings\n",
    "    word_index: word-to-index dictionary\n",
    "    \"\"\"\n",
    "    embedding_dim = 200\n",
    "    num_words = len(word_index)\n",
    "    \n",
    "    sequence_input = Input(\n",
    "      shape=(max_sequence_length,), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "      num_words, embedding_dim, weights=[embedding_matrix],\n",
    "      input_length=max_sequence_length, trainable=False)\n",
    "    \n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Conv1D(100, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(100, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    # x = keras.layers.Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "\n",
    "    return sequence_input, x, Model(sequence_input, x)\n",
    "\n",
    "\n",
    "import keras\n",
    "def full_model(ys_left, ys_right):\n",
    "    ys_merged = keras.layers.concatenate([ys_left, ys_right])\n",
    "    ys_out = keras.layers.Dense(100)(ys_merged)\n",
    "    return keras.Model(\n",
    "      inputs=[xs_left, xs_right], outputs=ys_merged)\n",
    "\n",
    "\n",
    "def get_network_lex_cues_pipe():\n",
    "    # TODO this can be done prior to iteration\n",
    "    # network_feat_pipeline is too slow\n",
    "    #net_pipe = network_feat_pipeline()\n",
    "    #net_pipe = pipeutil.pipe(\n",
    "    #    GlobalTransform(lambda df: df.body_atreplies))\n",
    "    \n",
    "    cues_pipe = pipeutil.pipe(\n",
    "        GlobalTransform(lambda df: df.body_content),\n",
    "        TokenizerTransformer().transform,\n",
    "        lexical_feat_pipeline())\n",
    "    return pipeutil.union(cues_pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loh = pd.get_dummies(df.label)\n",
    "df = df.assign(label_onehot=loh.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0, 1]), list([0, 1]), list([0, 1]), list([0, 1]),\n",
       "       list([0, 1])], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_onehot.head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_sequence(index, text):\n",
    "    return [index[w] for w in text if w in index]\n",
    "to_sequence({'a': 0, 'b': 1}, ['b', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_nl = get_network_lex_cues_pipe()\n",
    "#%time xs_nl = preprocess_nl.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_nl = get_network_lex_cues_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train stats\n",
      "1    9000\n",
      "0    9000\n",
      "Name: label, dtype: int64\n",
      "Test stats\n",
      "1    1000\n",
      "0    1000\n",
      "Name: label, dtype: int64\n",
      "Preprocessing\n",
      "Preprocessed network & lexical cues.\n",
      "[['embraer', 'tumbles', 'in', 'brazil', 'after', 'report', 'on', 'bribery', 'allegations', '@url', 'via', '@user'], ['@user', '@user', 'now', 'i', 'believed', 'that', 'many', 'of', 'the', 'pilipino', 'people', 'need', 'education']] [1 1]\n",
      "[['lloyd', 's', 'of', 'london', 'profits', 'fall', '30', '@url'], ['@user', '@user', 'क', 'य', 'भ', 'स', 'स', 'नफरत', 'घ', 'ड', 'स', 'प', 'य', 'र']] [1 1]\n",
      "Seq: [[7985, 24000, 11759, 3432, 1027, 19777, 16520, 3483, 1291, 558, 24791, 559], [559, 559, 16182, 11495, 2823, 23261, 14337, 16401, 23265, 17678, 17395, 15791, 7831]]\n",
      "Preprocessed CNN inputs, building model\n",
      "NC shape (2723,)\n",
      "Compiling model\n",
      "Starting training.\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 3s 147us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 2s 94us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 2s 95us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 2s 97us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 2s 115us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 2s 116us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 2s 103us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 2s 115us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 2s 134us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 2s 118us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "1\n",
      "Train stats\n",
      "1    9000\n",
      "0    9000\n",
      "Name: label, dtype: int64\n",
      "Test stats\n",
      "1    1000\n",
      "0    1000\n",
      "Name: label, dtype: int64\n",
      "Preprocessing\n",
      "Preprocessed network & lexical cues.\n",
      "[['lloyd', 's', 'of', 'london', 'profits', 'fall', '30', '@url'], ['@user', '@user', 'क', 'य', 'भ', 'स', 'स', 'नफरत', 'घ', 'ड', 'स', 'प', 'य', 'र']] [1 1]\n",
      "[['embraer', 'tumbles', 'in', 'brazil', 'after', 'report', 'on', 'bribery', 'allegations', '@url', 'via', '@user'], ['@user', '@user', 'now', 'i', 'believed', 'that', 'many', 'of', 'the', 'pilipino', 'people', 'need', 'education']] [1 1]\n",
      "Seq: [[13813, 20499, 16419, 13878, 18522, 8935, 325, 569], [570, 570, 28189, 28314, 28304, 28356, 28356, 28274, 28218, 28251, 28356, 28279, 28314, 28317]]\n",
      "Preprocessed CNN inputs, building model\n",
      "NC shape (2723,)\n",
      "Compiling model\n",
      "Starting training.\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 3s 152us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 2s 97us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 2s 104us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 2s 105us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 2s 100us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 2s 103us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 2s 126us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 2s 116us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 2s 128us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 2s 119us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "2\n",
      "Train stats\n",
      "1    9000\n",
      "0    9000\n",
      "Name: label, dtype: int64\n",
      "Test stats\n",
      "1    1000\n",
      "0    1000\n",
      "Name: label, dtype: int64\n",
      "Preprocessing\n",
      "Preprocessed network & lexical cues.\n",
      "[['lloyd', 's', 'of', 'london', 'profits', 'fall', '30', '@url'], ['@user', '@user', 'क', 'य', 'भ', 'स', 'स', 'नफरत', 'घ', 'ड', 'स', 'प', 'य', 'र']] [1 1]\n",
      "[['@user', 'i', 'doubt', 'it'], ['@user', '@user', '@user', 'en', 'todos', 'lados', 'hay', 'periodistas', 'mediocres', 'con', 'kk', 'en', 'la', 'cabeza', 'y', 'por', 'eso', 'no', 'razonan', 'sus', 'comentarios']] [1 1]\n",
      "Seq: [[13764, 20464, 16361, 13830, 18473, 8923, 319, 562], [563, 563, 28094, 28206, 28197, 28243, 28243, 28167, 28119, 28145, 28243, 28172, 28206, 28209]]\n",
      "Preprocessed CNN inputs, building model\n",
      "NC shape (2723,)\n",
      "Compiling model\n",
      "Starting training.\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 3s 175us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 2s 99us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " 6912/18000 [==========>...................] - ETA: 1s - loss: 7.9158 - acc: 0.5035"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-843fae37c945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m           validation_data=([xs_test_nl, xs_test_cnn], ys_test),)\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "df_ = df[df.label == 1].sample(n=10000).append(\n",
    "    df[df.label == 0].sample(n=10000))\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=const.SEED)\n",
    "\n",
    "for i, (itrain, itest) in enumerate(kfold.split(df_, y=df_.label)):\n",
    "    print(i)\n",
    "    \n",
    "    ltype = 'label'\n",
    "    df_train = df_.iloc[itrain]\n",
    "    ys_train = df_[ltype].iloc[itrain].values\n",
    "    df_test = df_.iloc[itest]\n",
    "    ys_test = df_[ltype].iloc[itest].values\n",
    "    \n",
    "    print(\"Train stats\\n%s\" % df_train.label.value_counts())\n",
    "    print(\"Test stats\\n%s\" % df_test.label.value_counts())\n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    preprocess_nl = get_network_lex_cues_pipe()\n",
    "    %time xs_train_nl = preprocess_nl.fit_transform(df_train)\n",
    "    %time xs_test_nl = preprocess_nl.transform(df_test)\n",
    "    \n",
    "    print(\"Preprocessed network & lexical cues.\")\n",
    "    lmap = lambda f, l: list(map(f, l))\n",
    "    tweet_tokenizer = TokenizerTransformer()\n",
    "    \n",
    "    xs_train_cnn = lmap(tweet_tokenizer.transform, df_train.body_content)\n",
    "    xs_test_cnn = lmap(tweet_tokenizer.transform, df_test.body_content)\n",
    "    \n",
    "    print(xs_train_cnn[:2], ys_train[:2])\n",
    "    print(xs_test_cnn[:2], ys_test[:2])\n",
    "\n",
    "    count_vectorizer = fe.text.CountVectorizer(\n",
    "        tokenizer=functions.id(),\n",
    "        analyzer=functions.id())\n",
    "    count_vectorizer.fit(xs_train_cnn)\n",
    "    \n",
    "    to_seq_vocab = lambda l: lmap(\n",
    "        lambda x: to_sequence(count_vectorizer.vocabulary_, x), l)\n",
    "    print(\"Seq:\", to_seq_vocab(xs_train_cnn[:2]))\n",
    "    \n",
    "    max_seq_len = text_lens.max()\n",
    "    xs_train_cnn = pad_sequences(\n",
    "        to_seq_vocab(xs_train_cnn), maxlen=max_seq_len, padding='post')\n",
    "    xs_test_cnn = pad_sequences(\n",
    "        to_seq_vocab(xs_test_cnn), maxlen=max_seq_len, padding='post')\n",
    "    \n",
    "    embedding_matrix = gloveutil.glove_to_embedding_matrix(\n",
    "        glove_200, count_vectorizer.vocabulary_)\n",
    "    \n",
    "    print(\"Preprocessed CNN inputs, building model\")\n",
    "    print(\"NC shape\", xs_train_nl.shape[1:])\n",
    "    nl_in, nl_out, model_nl = network_cues_model(xs_train_nl.shape[1:])\n",
    "    cnn_in, cnn_out, model_cnn = sequence_model(\n",
    "        embedding_matrix, count_vectorizer.vocabulary_, max_seq_len)\n",
    "    \n",
    "    out_merged = keras.layers.concatenate([nl_out, cnn_out])\n",
    "    out = Dense(1, activation='softmax')(nl_out) #(out_merged)\n",
    "    model = keras.Model(\n",
    "      inputs=[nl_in, cnn_in], outputs=out)\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    print(\"Compiling model\")\n",
    "    model.compile(\n",
    "      optimizer=keras.optimizers.SGD(),\n",
    "      loss=keras.losses.binary_crossentropy,\n",
    "      metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Starting training.\")\n",
    "    model.fit([xs_train_nl, xs_train_cnn], ys_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=([xs_test_nl, xs_test_cnn], ys_test),)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16462,  4546, 21705, ...,     0,     0,     0],\n",
       "       [  563,   707, 24040, ...,     0,     0,     0],\n",
       "       [  563,   563, 25719, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  563, 11038, 20568, ...,     0,     0,     0],\n",
       "       [12736, 17572, 11009, ...,     0,     0,     0],\n",
       "       [22912, 12587, 22330, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_test_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 53)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 53, 200)      895800      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 49, 100)      100100      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 9, 100)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 806)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5, 100)       50100       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          80700       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          10100       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          10100       global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200)          0           dense_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            402         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,147,302\n",
      "Trainable params: 251,502\n",
      "Non-trainable params: 895,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "test_model = Sequential()\n",
    "test_model.add(Dense(200, input_shape=(200,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
